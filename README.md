<div align="center">
<img width="1200" height="475" alt="GHBanner" src="image.png" />
</div>


# ðŸ§  AI Incident Intelligence Platform

> **AI-powered platform for analyzing system incidents and technical documents using Gemini LLM and Retrieval-Augmented Generation (RAG).**

AI Incident Intelligence is a full-stack, production-ready AI system that allows users to upload PDFs (logs, incident reports, manuals) and ask intelligent questions about them.
The system uses **Gemini LLM**, **vector search (FAISS)**, and **RAG** to provide grounded, context-aware answers.

---

##  Features

* ðŸ“„ Upload and index multiple PDF documents
* ðŸ” Semantic search using vector embeddings
* ðŸ¤– Gemini LLM for reasoning and answers
* ðŸ§  Retrieval-Augmented Generation (RAG)
* ðŸ’¬ Chat-based UI (React + TypeScript)
* ðŸ³ Fully Dockerized backend
* ðŸŒ Browser-ready (CORS enabled)
* â˜ï¸ Cloud-ready architecture (AWS / GCP / any VM)

---

## ðŸ—ï¸ System Architecture

```
Frontend (React / Vite)
        |
        v
FastAPI Backend (Docker)
        |
        â”œâ”€â”€ PDF Upload
        â”œâ”€â”€ Text Chunking
        â”œâ”€â”€ Embeddings
        â”œâ”€â”€ FAISS Vector DB
        â””â”€â”€ Gemini LLM API
```

---

## ðŸ› ï¸ Tech Stack

### Backend

* Python
* FastAPI
* Gemini API (Google)
* FAISS (Vector DB)
* Sentence Transformers
* PyPDF2

### Frontend

* React + TypeScript
* Vite
* Modern SaaS UI

### Infrastructure

* Docker
* Docker Compose
* Environment variables
* Volumes for persistence

---

## ðŸ“ Project Structure

```
ai-incident-intelligence/
â”œâ”€â”€ backend/           # FastAPI + RAG logic
â”œâ”€â”€ frontend/          # React UI
â”œâ”€â”€ data/pdfs/         # Uploaded PDFs
â”œâ”€â”€ vector_store/      # FAISS index
â”œâ”€â”€ docker-compose.yml
â””â”€â”€ README.md
```
## Full PROJECT STRUCTURE
``` ai-incident-intelligence
â”œâ”€â”€ backend
â”‚Â Â  â”œâ”€â”€ Dockerfile
â”‚Â Â  â”œâ”€â”€ embeddings.py
â”‚Â Â  â”œâ”€â”€ gemini.py
â”‚Â Â  â”œâ”€â”€ main.py
â”‚Â Â  â”œâ”€â”€ __pycache__
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ gemini.cpython-310.pyc
â”‚Â Â  â”‚Â Â  â””â”€â”€ main.cpython-310.pyc
â”‚Â Â  â”œâ”€â”€ rag.py
â”‚Â Â  â”œâ”€â”€ requirements.txt
â”‚Â Â  â””â”€â”€ test.py
â”œâ”€â”€ components
â”‚Â Â  â”œâ”€â”€ ChatWindow.tsx
â”‚Â Â  â””â”€â”€ Sidebar.tsx
â”œâ”€â”€ data
â”‚Â Â  â””â”€â”€ pdfs
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ frontend
â”‚Â Â  â”œâ”€â”€ App.tsx
â”‚Â Â  â”œâ”€â”€ components
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ ChatWindow.tsx
â”‚Â Â  â”‚Â Â  â””â”€â”€ Sidebar.tsx
â”‚Â Â  â”œâ”€â”€ index.html
â”‚Â Â  â”œâ”€â”€ index.tsx
â”‚Â Â  â”œâ”€â”€ metadata.json
â”‚Â Â  â”œâ”€â”€ package.json
â”‚Â Â  â”œâ”€â”€ README.md
â”‚Â Â  â”œâ”€â”€ services
â”‚Â Â  â”‚Â Â  â””â”€â”€ api.ts
â”‚Â Â  â”œâ”€â”€ tsconfig.json
â”‚Â Â  â”œâ”€â”€ types.ts
â”‚Â Â  â””â”€â”€ vite.config.ts
â”œâ”€â”€ README.md
â”œâ”€â”€ services
â”‚Â Â  â””â”€â”€ api.ts
â””â”€â”€ vector_store
    â”œâ”€â”€ index.faiss
    â””â”€â”€ meta.pkl
```
---

##  Setup & Run (Local)

### 1. Clone the repo

```bash
git clone https://github.com/your-username/ai-incident-intelligence.git
cd ai-incident-intelligence
```

### 2. Add Gemini API key

Create a `.env` file:

```env
GEMINI_API_KEY=your_api_key_here
```

### 3. Run with Docker

```bash
docker compose up --build
```

Backend will be available at:

```
http://localhost:8000
```

API docs:

```
http://localhost:8000/docs
```

Frontend:

```
http://localhost:5173
```

---

##  API Endpoints

### Health

```http
GET /health
```

### Upload PDF

```http
POST /upload
Content-Type: multipart/form-data
```

### Ask Question

```http
POST /ask
Content-Type: application/json

{
  "question": "What is this document about?"
}
```

---

##  How RAG Works

1. User uploads PDF
2. Text is extracted and chunked
3. Each chunk is embedded into vectors
4. Vectors stored in FAISS
5. User asks a question
6. Relevant chunks retrieved
7. Gemini answers using retrieved context

This ensures:

* Lower API cost
* Better accuracy
* No hallucinations

---

##  Example Use Cases

* Analyze system error logs
* Search technical documentation
* AI DevOps assistant
* Incident root cause analysis
* Internal knowledge base

---

##  Example cURL Test

Upload PDF:

```bash
curl -X POST http://localhost:8000/upload \
  -F "file=@example.pdf"
```

Ask question:

```bash
curl -X POST http://localhost:8000/ask \
  -H "Content-Type: application/json" \
  -d '{"question":"Summarize this document"}'
```

---

##  Deployment

This system can be deployed to:

* AWS EC2
* Google Cloud VM
* DigitalOcean
* Any Linux server

Steps:

```bash
sudo apt install docker.io docker-compose -y
git clone your-repo
cd ai-incident-intelligence
docker compose up -d
```

---

##  Security Notes

* API keys are stored in environment variables
* `.env` should never be committed
* In production, restrict CORS and use HTTPS

---

##  Future Improvements

* Multi-user authentication
* Streaming responses
* Cost monitoring dashboard
* Source citation per answer
* Redis caching
* Pinecone / Weaviate support

---

##  Author

**Mohammed Kahab K**
AI / ML Engineer

> This project demonstrates real-world skills in:
>
> * Applied AI engineering
> * RAG systems
> * Cloud LLM integration
> * Dockerized production systems

---


